# SparseFlow SPA Overview

## What is SPA?

**SPA (Sparsity Propagation Analysis)** is an MLIR compiler pass that statically analyzes structured sparsity patterns in tensor operations and generates runtime-ready metadata for optimization.

## The Problem

Modern ML models contain significant structured sparsity:
- N:M patterns (e.g., 2:4, 4:8) from pruning
- Block sparsity from quantization
- Channel sparsity from architecture search

But current frameworks either:
- **Ignore it** (waste computation on zeros)
- **Detect it at runtime** (overhead + latency)
- **Require manual annotation** (error-prone)

## SparseFlow's Solution

### Pipeline Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MLIR Source â”‚ (e.g., linalg.matmul)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPA Pass   â”‚ Detects: rowmask=[T,F,T,F]
â”‚   (v0.6)    â”‚         colmask=[T,T,F,F]
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JSON Export â”‚ spa_sparsity.json
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  C++ Runtimeâ”‚ OpenMP masked matmul
â”‚  (OpenMP)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ~4Ã— Speedupâ”‚ ğŸ”¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Concrete Example: 512Ã—512 Matmul

**Input Pattern:** 50% row sparsity + 50% column sparsity

**SPA Analysis:**
```mlir
linalg.matmul {
  sparseflow.spa_rowmask = [true, false, true, false, ...],
  sparseflow.spa_colmask = [true, true, false, false, ...]
}
```

**JSON Export:**
```json
{
  "id": 0,
  "name": "linalg.matmul",
  "row_sparsity_pct": 50,
  "col_sparsity_pct": 50,
  "total_rows": 512,
  "total_cols": 512
}
```

**Runtime Performance:**
- Dense baseline: 336 ms
- SPA-optimized: 101 ms
- **Speedup: 3.31Ã—** âœ…

**FLOP Reduction:**
- Original: 512Ã—512Ã—512 = 134M FLOPs
- Active: 256Ã—256Ã—512 = 33M FLOPs  
- **Reduction: 75%** (matches speedup!)

## Current Results

| Matrix Size | Speedup | Environment |
|-------------|---------|-------------|
| 256Ã—256     | 4.33Ã—   | Codespaces  |
| 512Ã—512     | 3.31Ã—   | Codespaces  |
| 768Ã—768     | 4.77Ã—   | Codespaces  |
| 1024Ã—1024   | 4.31Ã—   | Codespaces  |

**Average:** ~4Ã— (consistent with 75% FLOP reduction)

## What's Unique

1. **Static Analysis:** Detects sparsity at compile-time (no runtime overhead)
2. **2D Tracking:** Tracks both row and column sparsity (not just 1D)
3. **MLIR Integration:** Works with standard compiler infrastructure
4. **Proven Results:** Reproducible 4Ã— speedup on real hardware

## Current Limitations

- **CPU-only:** No GPU kernels yet
- **Manual Integration:** JSON â†’ Runtime bridge is manual
- **Limited Ops:** Only matmul fully supported
- **No Framework Support:** No PyTorch/TensorRT integration

## Roadmap

**Phase 1 (âœ… Complete):** Static analysis + CPU runtime  
**Phase 2 (Next):** GPU acceleration (CUDA/ROCm)  
**Phase 3 (Future):** Framework integration (PyTorch/ONNX)  
**Phase 4 (Research):** Dynamic sparsity tracking

