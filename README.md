# ğŸŒ² SparseFlow v0.2.0  
### Generalized N:M Sparse Compiler for AI Inference (MLIR + CPU Runtime)

SparseFlow is a next-generation MLIR-based compiler that detects and exploits **generalized structured sparsity (N:M)** in AI workloads.

Unlike traditional sparse libraries (limited to 2:4 or fully unstructured), SparseFlow supports **any N:M block pattern** and achieves **massive CPU acceleration** using compile-time analysis + custom sparse kernels.

---

## ğŸš€ Key Features (v0.2.0)

### âœ… Generalized N:M Sparsity  
Supports the following patterns out of the box:

- 1:4  
- 2:4  
- 2:8  
- 4:16  
- 8:32  

### âœ… MLIR Compiler Integration  
- SPA Pass â€” Static sparsity analysis  
- Rewrite Pass â€” Converts dense matmuls â†’ sparse kernels  
- Export Pass â€” Dumps metadata  
- Pluggable runtime lowering

### âœ… Optimized CPU Runtime  
- 5 hand-tuned OpenMP kernels  
- Contiguous block loads  
- Branch-free inner loops  
- High cache locality  
- Designed for future SIMD + GPU backend

### âœ… Real Performance  
SparseFlow achieves **9Ã—â€“20Ã— speedup** on CPU for realistic matrix sizes, significantly outperforming typical sparse CPU libraries.

---

## ğŸ“Š Benchmark Results (REAL HARDWARE)

Benchmarks compare dense vs SparseFlow sparse kernels on CPU.

| Matrix Size | Typical Speedup | Peak Speedup |
|-------------|------------------|----------------|
| **256Ã—256** | 3Ã—â€“8Ã— | 8Ã— |
| **512Ã—512** | 8Ã—â€“12Ã— | 12Ã— |
| **1024Ã—1024** | 9Ã—â€“20Ã— | 20Ã— |

Stable patterns frequently hit:

- **1:4 â†’ ~18Ã—**
- **2:8 â†’ ~18Ã—**
- **4:16 â†’ ~20Ã—**

These numbers are based on multiple runs and exclude outlier spikes.

---

## ğŸ§ª Example Benchmark Output
```
Matrix Size: 1024Ã—1024
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pattern â”‚ Dense (ms) â”‚ Sparse (ms)â”‚ Speedup  â”‚ Density   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1:4     â”‚ 12618.09   â”‚ 670.56     â”‚ 18.82Ã—   â”‚ 25%       â”‚
â”‚ 2:4     â”‚ 14662.58   â”‚ 1626.62    â”‚ 9.01Ã—    â”‚ 50%       â”‚
â”‚ 2:8     â”‚ 13843.85   â”‚ 769.59     â”‚ 17.99Ã—   â”‚ 25%       â”‚
â”‚ 4:16    â”‚ 10886.07   â”‚ 544.07     â”‚ 20.01Ã—   â”‚ 25%       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ— Compiler Pipeline

SparseFlow transforms dense MLIR into sparse-optimized executable code:
```
PyTorch / ONNX â†’ MLIR â†’ SPA Pass â†’ Rewrite Pass â†’ LLVM â†’ Sparse Runtime
```

### 1. SPA Pass  
Identifies sparse regions and marks tensors with `{n, m}` metadata.

### 2. Rewrite Pass  
Replaces `linalg.matmul` with:
```mlir
func.call @sparse_matmul_N_M(...)
```

Dynamically choosing the correct sparse kernel.

### 3. Runtime  
Backed by optimized C++/OpenMP kernels:
```cpp
sparse_matmul_1_4
sparse_matmul_2_4
sparse_matmul_2_8
sparse_matmul_4_16
sparse_matmul_8_32
```

---

## ğŸ§© Supported Sparsity Patterns

A pattern **N:M** means:

- For every M consecutive weights  
- Exactly N are non-zero  
- Zeros are static at compile time  
- Blocks are memory contiguous  

This allows:

- Predictable skipping  
- SIMD-friendly loads  
- Low branch divergence  
- Great cache efficiency  

---

## ğŸ”¬ Example MLIR Input
```mlir
%A = tensor<16x16xf32> {n = 2 : i32, m = 8 : i32}
%B = tensor<16x16xf32>
%C = tensor<16x16xf32>

%0 = linalg.matmul ins(%A, %B)
```

### After Rewrite Pass:
```mlir
func.call @sparse_matmul_2_8(%A, %B, %C, %m, %k, %n)
```

---

## ğŸ“¦ Build Instructions
```bash
git clone https://github.com/MapleSilicon/SparseFlow
cd SparseFlow/compiler
mkdir build && cd build
cmake -DCMAKE_PREFIX_PATH=/usr/lib/llvm-19 ..
make -j8
```

### Run benchmarks
```bash
cd ../../runtime/build
./benchmark_nm_runtime
```

---

## ğŸ—º Roadmap

### **v0.3 (Q1 2026) â€” GPU Acceleration**
* CUDA kernels
* Tensor Core support
* 30â€“60Ã— expected speedup

### **v0.4 (Q2 2026) â€” PyTorch Integration**
* Python bindings
* `torch.compile` backend
* Model zoo support

### **v0.5 (Q3 2026) â€” Production Deployment**
* Cloud provider pilots
* Enterprise safety and tooling

---

## ğŸ¤ Contact

**Email:** maplesilicon1@gmail.com  
**GitHub:** https://github.com/MapleSilicon/SparseFlow  
**Author:** Gourav Kumar

---

# ğŸŒ² SparseFlow

**Generalized Sparse Compute for AI.**  
**Simple. Fast. Open.**
